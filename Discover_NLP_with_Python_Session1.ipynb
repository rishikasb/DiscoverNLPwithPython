{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Discover NLP with Python - Session1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob5fTzLcJSnj"
      },
      "source": [
        "# Discover NLP with Python: Session #1 - NLP Foundations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLGeEeiIRXkb"
      },
      "source": [
        "Dataset Link: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
        "\n",
        "Dataset Name: 'SMSSpamcollection.zip' (https://archive.ics.uci.edu/ml/machine-learning-databases/00228/)\n",
        "\n",
        "Dataset Description: a public set of SMS labeled messages that have been collected for mobile phone spam research.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTzmFo59Rt4Y"
      },
      "source": [
        "**Steps to access, download, and use the dataset:**\n",
        "\n",
        "- Access the above link to the UCI ML Repository, click on \"smsspamcollection.zip\". \n",
        "- Once downloaded, click on the file to unzip them. Make sure to save the file in a folder easily accessible - E.g. \"Documents\" or \"Desktop\"\n",
        "- Open a Google Colab Notebook:\n",
        "  - In the left pane of the Google Colab, click on \"Files\" and select the upload icon (Icon hover text: \"Upload to session storage\")\n",
        "  - You are now ready to start using the data in the Colab Notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YxeGiHXSB65"
      },
      "source": [
        "Data location within Colab: '/content/SMSSpamCollection'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A9cFPtUSaST"
      },
      "source": [
        "Check whick folder in the Google Colab notebook we are currently in?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj8QHG78SbiU",
        "outputId": "b3db01f7-2a6d-4a88-de5d-6959d55b63b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pwd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wehxiG6OSckC"
      },
      "source": [
        "Now that we know which folder we are in, how do we see if the data we loaded is correctly loaded or not?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGBzKhmsScR2",
        "outputId": "eaf02895-1bd3-4027-a489-075c6c8173d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/  SMSSpamCollection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq7zWODDLXQz"
      },
      "source": [
        "First, you import the Pandas package as pd. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExCELPIgLXet"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RtrI7ECLdI5"
      },
      "source": [
        "And then, we use the read_csv() function for loading in the data. We pass into this function the URL in which the data can be found. To make it easier to work with the data in the future, we will name it by assigning it to a variable (digits in this case). \n",
        "\n",
        "Note: we use \"header = None\" here so that the first row of our data will not be interpreted as the column names of the data frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlCiEGN0Lddx"
      },
      "source": [
        "sms = pd.read_table('/content/SMSSpamCollection', header=None)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMkq2sXeL41o"
      },
      "source": [
        "In order to see what data we loaded in, we can simply do this below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "654CaLnWL5Jz",
        "outputId": "b43854d8-190f-4ca9-d608-663a5de7671a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "print(sms)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         0                                                  1\n",
            "0      ham  Go until jurong point, crazy.. Available only ...\n",
            "1      ham                      Ok lar... Joking wif u oni...\n",
            "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3      ham  U dun say so early hor... U c already then say...\n",
            "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
            "...    ...                                                ...\n",
            "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
            "5568   ham               Will Ã¼ b going to esplanade fr home?\n",
            "5569   ham  Pity, * was in mood for that. So...any other s...\n",
            "5570   ham  The guy did some bitching but I acted like i'd...\n",
            "5571   ham                         Rofl. Its true to its name\n",
            "\n",
            "[5572 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrs6YChqCsg",
        "outputId": "b2c0bd56-fc8f-433e-92f1-87efcbcba099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "sms.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0                                                  1\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEDEHD0oJXzD"
      },
      "source": [
        "### Describe the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QISsuHR5Mlj_"
      },
      "source": [
        "One of the most elementary steps to do this is by getting a basic description of your data. A basic description of your data is indeed a very broad term: you can interpret it as a quick and dirty way to get some information on your data, as a way of getting some simple, easy-to-understand information on your data, to get a basic feel for your data, etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7GI-NSJMypB"
      },
      "source": [
        "To begin, we can use the describe() function to obtain various summary statistics that exclude NaN values. We can refer back to our digits data and understand it a bit better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpzktpYPLIei",
        "outputId": "8bb66a3a-8cdc-48ad-98ed-f6f3a158ab26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "sms.describe()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5572</td>\n",
              "      <td>5572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>2</td>\n",
              "      <td>5169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>ham</td>\n",
              "      <td>Sorry, I'll call later</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>4825</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           0                       1\n",
              "count   5572                    5572\n",
              "unique     2                    5169\n",
              "top      ham  Sorry, I'll call later\n",
              "freq    4825                      30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucNsl21IM_W4"
      },
      "source": [
        "You see that this function returns the count, mean, standard deviation, minimum and maximum values and the quantiles of the data. Note that, of course, there are many packages available in Python that can give you those statistics, including Pandas itself. Using this function is just one of the ways to get this information. You can use these descriptive statistics to begin to assess the quality of your data. Then youâll be able to decide whether you need to correct, discard or deal with the data in anohter way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyubgQxRK7aU"
      },
      "source": [
        "### Data Info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoBb6vu9NLbz"
      },
      "source": [
        "Here, we have a collection of text data known as a corpus. Specifically, there are 5,572 SMS messages written in English, serving as training examples. The first column is the target variable containing the class labels, which tells us if the message is spam or ham (aka not spam). The second column is the SMS message itself, stored as a string."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM6wT7MWWLdG"
      },
      "source": [
        "Since the target variable contains discrete values, this is a **classification** task. Let's start by placing the target variable in its own table and checking out how the two classes are distributed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL77rJf7NfVW",
        "outputId": "26980bbe-3a70-4bfa-ae46-6877f7beedae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "y = sms[0]\n",
        "y.value_counts()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ham     4825\n",
              "spam     747\n",
              "Name: 0, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPrTvN3mNNOq"
      },
      "source": [
        "It looks like there are far fewer training examples for spam than hamâwe'll take this imbalance into account in the analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8CwsiEsWdD-"
      },
      "source": [
        "In addition, we need to encode the class labels in the target variable as numbers to ensure compatibility with some models in Scikit-learn. Because we have binary classes, let's use LabelEncoder and set 'spam' = 1 and 'ham' = 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08MzZah8XI4e"
      },
      "source": [
        "LabelEncoder is a function of scikit learn's preprocessing capabilities, which helps to encode target labsls with values between 0 and the (# of classes) - 1. \n",
        "\n",
        "Note: Refer to this website for more information (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJxyWauaXInG"
      },
      "source": [
        "from sklearn import preprocessing"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWm_nsPzLFlN"
      },
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "y_enc = le.fit_transform(y)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtX_JCKBXjZ7",
        "outputId": "81ed49e0-119a-47b6-d53c-d1d9457ceabb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_enc"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxdAQXrjWkr1"
      },
      "source": [
        "Next, we place the SMS message data into its own table. We must convert this corpus into useful numerical features so we can train this classifier and this is where NLP works its magic!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PYEcisJW8zG"
      },
      "source": [
        "raw_text = sms[1]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6UsVe0OW69q",
        "outputId": "dfb7b390-45fd-466f-a7c3-b22370375cf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "raw_text"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Go until jurong point, crazy.. Available only ...\n",
              "1                           Ok lar... Joking wif u oni...\n",
              "2       Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3       U dun say so early hor... U c already then say...\n",
              "4       Nah I don't think he goes to usf, he lives aro...\n",
              "                              ...                        \n",
              "5567    This is the 2nd time we have tried 2 contact u...\n",
              "5568                 Will Ã¼ b going to esplanade fr home?\n",
              "5569    Pity, * was in mood for that. So...any other s...\n",
              "5570    The guy did some bitching but I acted like i'd...\n",
              "5571                           Rofl. Its true to its name\n",
              "Name: 1, Length: 5572, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIdoXqLaPY7O"
      },
      "source": [
        "Another important part of any dataset is missing values. When this happens, the dataset can lose expressiveness, which may lead to weak or at times biased analyses. Practically, this means that when youâre missing values for certain features, the chances of your classification or predictions for the data being off only increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwXB_McsPjqC"
      },
      "source": [
        "To identify the rows that contain missing values, you can use isnull(). In the result that youâll get back, youâll see True or False appearing in each cell: True will indicate that the value contained within the cell is a missing value, False means that the cell contains a ânormalâ value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvdbliy1PkCt",
        "outputId": "863bd881-e3d0-4901-9553-bf957e9f48c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "pd.isnull(sms)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows Ã 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0      1\n",
              "0     False  False\n",
              "1     False  False\n",
              "2     False  False\n",
              "3     False  False\n",
              "4     False  False\n",
              "...     ...    ...\n",
              "5567  False  False\n",
              "5568  False  False\n",
              "5569  False  False\n",
              "5570  False  False\n",
              "5571  False  False\n",
              "\n",
              "[5572 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHLGVRvBPuox"
      },
      "source": [
        "In this case, you see that the data is quite complete: there are no missing values and you're lucky! But this is definitely not always the case for datasets out there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZgFXvPeK731"
      },
      "source": [
        "### Basic Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLqnQ-KuQEu7"
      },
      "source": [
        "Data visualization can help with identifying patterns in the data. The Python libraries Seaborn and Matplotlib are easy and quick ways to do this. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Smz8QSJBLEJg",
        "outputId": "5ab14137-aa26-4aeb-ac33-3d3c5ccdead0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import matplotlib as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52s9-Oc1XvgX"
      },
      "source": [
        "There are a couple basic visualizations we can do. The first is displaying the length of all the dataset instances. To do this, we must first label the columns with their appropriate titles and add a column to the dataset that contains the length. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD-0ZhgOqPZ5"
      },
      "source": [
        "sms.columns=['label', 'msg']"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeEVeKxZq2c3",
        "outputId": "17b07372-73d2-427a-e99f-15678558d26b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "sms.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                                msg\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f02B7Y6Y--F"
      },
      "source": [
        "sms[\"length\"] = sms[\"msg\"].apply(len)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZvGSe3Kq9Q9",
        "outputId": "70074ac2-3b97-437b-a78b-6e8b75d2b0cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "sms.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                                msg  length\n",
              "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
              "1   ham                      Ok lar... Joking wif u oni...      29\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
              "3   ham  U dun say so early hor... U c already then say...      49\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tGjoINoXvJM",
        "outputId": "a5a53875-d4f1-488c-fbaf-4b3e07b0ffca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "sns.distplot(sms[\"length\"], kde=False)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f3911df2dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X0lTCL0KFqk"
      },
      "source": [
        "References:\n",
        "- https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/\n",
        "- https://www.datacamp.com/community/tutorials/exploratory-data-analysis-python\n",
        "- https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7Sal7tUXVBj"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZEmaQyrLIzI"
      },
      "source": [
        "## Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXeOxA2vJowM"
      },
      "source": [
        "### Step 1: Contraction Mapping / Expanding Contractions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfS7b5XQ6zZK"
      },
      "source": [
        "First, we install and import the necessary library - contractions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bIyOL3A3sZz",
        "outputId": "724e7051-e1d7-4892-c1c0-c4b92ec62add",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        }
      },
      "source": [
        "!pip install contractions\n",
        "import contractions"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/00/92/a05b76a692ac08d470ae5c23873cf1c9a041532f1ee065e74b374f218306/contractions-0.0.25-py2.py3-none-any.whl\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 245kB 3.5MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 317kB 14.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81692 sha256=6784eaf0a1645840193ad44dfd0f8b45a820d68af91b52555d3315579543f20e\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: Unidecode, pyahocorasick, textsearch, contractions\n",
            "Successfully installed Unidecode-1.1.1 contractions-0.0.25 pyahocorasick-1.4.0 textsearch-0.0.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEKvXDOH31l7"
      },
      "source": [
        "sms['no_contract'] = sms['msg'].apply(lambda x: [contractions.fix(word) for word in x.split()])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRud_Vrk7Ggf",
        "outputId": "bbeb2a6c-7701-469c-abd2-80c26d2f1a0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "sms.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "      <th>length</th>\n",
              "      <th>no_contract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>111</td>\n",
              "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>29</td>\n",
              "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>155</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>49</td>\n",
              "      <td>[you, dun, say, so, early, hor..., you, c, alr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>61</td>\n",
              "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                        no_contract\n",
              "0   ham  ...  [Go, until, jurong, point,, crazy.., Available...\n",
              "1   ham  ...             [Ok, lar..., Joking, wif, you, oni...]\n",
              "2  spam  ...  [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
              "3   ham  ...  [you, dun, say, so, early, hor..., you, c, alr...\n",
              "4   ham  ...  [Nah, I, do not, think, he, goes, to, usf,, he...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLGxmOZI7NNn"
      },
      "source": [
        "Also, we would want the expanded contractions to be tokenized separately, therefore we convert the lists under the \"no_contract\" column back into strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzTj7U_J7USX"
      },
      "source": [
        "sms[\"msg_str\"] = [' '.join(map(str, l)) for l in sms['no_contract']]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO4OPskH7gRf",
        "outputId": "e2e765f3-780a-440d-a309-c1b6b847da3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "sms.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "      <th>length</th>\n",
              "      <th>no_contract</th>\n",
              "      <th>msg_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>111</td>\n",
              "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>29</td>\n",
              "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
              "      <td>Ok lar... Joking wif you oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>155</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>49</td>\n",
              "      <td>[you, dun, say, so, early, hor..., you, c, alr...</td>\n",
              "      <td>you dun say so early hor... you c already then...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>61</td>\n",
              "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
              "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                            msg_str\n",
              "0   ham  ...  Go until jurong point, crazy.. Available only ...\n",
              "1   ham  ...                    Ok lar... Joking wif you oni...\n",
              "2  spam  ...  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  ...  you dun say so early hor... you c already then...\n",
              "4   ham  ...  Nah I do not think he goes to usf, he lives ar...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xJy-B1G3ZSC"
      },
      "source": [
        "Reference: https://towardsdatascience.com/preprocessing-text-data-using-python-576206753c28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fD7Vjr8JlEI"
      },
      "source": [
        "### Step 2: Tokenization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw_4LrBeuOAn"
      },
      "source": [
        "We will begin my installing and importing nltk, so we can use it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCm7QGpCTnld",
        "outputId": "6ca411f0-0c73-48d0-df05-7c19ace453a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkE5LU-WvkAK"
      },
      "source": [
        "First, we will take a basic sentence to demonstrate this tokenization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVAOYdROv3u7"
      },
      "source": [
        "text = \"Hi, I would like to tokenize this sentence.\""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcfmSnNfTxBs",
        "outputId": "663256b7-3c1b-4ac8-a2d6-fe4c221018d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(word_tokenize(text))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hi', ',', 'I', 'would', 'like', 'to', 'tokenize', 'this', 'sentence', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP2etJNC-a-E"
      },
      "source": [
        "Now, we can apply the tokenizer to our dataset. We will apply NLTK.word_tokenize() function to the âmsg_strâ column and create a new column named âtokenizedâ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXKZEWZG-gII",
        "outputId": "cacb275c-16db-4466-ccc3-05d106accd77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "sms['tokenized'] = sms['msg_str'].apply(word_tokenize)\n",
        "sms.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "      <th>length</th>\n",
              "      <th>no_contract</th>\n",
              "      <th>msg_str</th>\n",
              "      <th>tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>111</td>\n",
              "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>[Go, until, jurong, point, ,, crazy.., Availab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>29</td>\n",
              "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
              "      <td>Ok lar... Joking wif you oni...</td>\n",
              "      <td>[Ok, lar, ..., Joking, wif, you, oni, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>155</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>49</td>\n",
              "      <td>[you, dun, say, so, early, hor..., you, c, alr...</td>\n",
              "      <td>you dun say so early hor... you c already then...</td>\n",
              "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>61</td>\n",
              "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
              "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
              "      <td>[Nah, I, do, not, think, he, goes, to, usf, ,,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                          tokenized\n",
              "0   ham  ...  [Go, until, jurong, point, ,, crazy.., Availab...\n",
              "1   ham  ...         [Ok, lar, ..., Joking, wif, you, oni, ...]\n",
              "2  spam  ...  [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
              "3   ham  ...  [you, dun, say, so, early, hor, ..., you, c, a...\n",
              "4   ham  ...  [Nah, I, do, not, think, he, goes, to, usf, ,,...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnYvZAOj23RF"
      },
      "source": [
        "Reference: https://www.guru99.com/tokenize-words-sentences-nltk.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dSlUTVNJjAU"
      },
      "source": [
        "### Step 3: Noise Cleaning - spacing, special characters, lowercasing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eTvf_eKrrEw"
      },
      "source": [
        "Let'a take a small step back and examine a few random examples of SMS messages from our dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZAoKyvnrxmB",
        "outputId": "59f94260-f6f9-4c5a-e95c-90c86baf9354",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "sms.sample(frac=0.05)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "      <th>length</th>\n",
              "      <th>no_contract</th>\n",
              "      <th>msg_str</th>\n",
              "      <th>tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1152</th>\n",
              "      <td>ham</td>\n",
              "      <td>Sorry, I'll call later</td>\n",
              "      <td>22</td>\n",
              "      <td>[Sorry,, I will, call, later]</td>\n",
              "      <td>Sorry, I will call later</td>\n",
              "      <td>[Sorry, ,, I, will, call, later]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1780</th>\n",
              "      <td>spam</td>\n",
              "      <td>Loan for any purpose Â£500 - Â£75,000. Homeowner...</td>\n",
              "      <td>162</td>\n",
              "      <td>[Loan, for, any, purpose, Â£500, -, Â£75,000., H...</td>\n",
              "      <td>Loan for any purpose Â£500 - Â£75,000. Homeowner...</td>\n",
              "      <td>[Loan, for, any, purpose, Â£500, -, Â£75,000, .,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>901</th>\n",
              "      <td>ham</td>\n",
              "      <td>Probably money worries. Things are coming due ...</td>\n",
              "      <td>126</td>\n",
              "      <td>[Probably, money, worries., Things, are, comin...</td>\n",
              "      <td>Probably money worries. Things are coming due ...</td>\n",
              "      <td>[Probably, money, worries, ., Things, are, com...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3786</th>\n",
              "      <td>ham</td>\n",
              "      <td>Let me know if you need anything else. Salad o...</td>\n",
              "      <td>98</td>\n",
              "      <td>[Let, me, know, if, you, need, anything, else....</td>\n",
              "      <td>Let me know if you need anything else. Salad o...</td>\n",
              "      <td>[Let, me, know, if, you, need, anything, else,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2396</th>\n",
              "      <td>ham</td>\n",
              "      <td>Babe, I'm back ... Come back to me ...</td>\n",
              "      <td>38</td>\n",
              "      <td>[Babe,, I am, back, ..., Come, back, to, me, ...]</td>\n",
              "      <td>Babe, I am back ... Come back to me ...</td>\n",
              "      <td>[Babe, ,, I, am, back, ..., Come, back, to, me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2708</th>\n",
              "      <td>spam</td>\n",
              "      <td>Great NEW Offer - DOUBLE Mins &amp; DOUBLE Txt on ...</td>\n",
              "      <td>160</td>\n",
              "      <td>[Great, NEW, Offer, -, DOUBLE, Mins, &amp;, DOUBLE...</td>\n",
              "      <td>Great NEW Offer - DOUBLE Mins &amp; DOUBLE Txt on ...</td>\n",
              "      <td>[Great, NEW, Offer, -, DOUBLE, Mins, &amp;, DOUBLE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5026</th>\n",
              "      <td>ham</td>\n",
              "      <td>I was gonna ask you lol but i think its at 7</td>\n",
              "      <td>44</td>\n",
              "      <td>[I, was, going to, ask, you, lol, but, i, thin...</td>\n",
              "      <td>I was going to ask you lol but i think its at 7</td>\n",
              "      <td>[I, was, going, to, ask, you, lol, but, i, thi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4456</th>\n",
              "      <td>ham</td>\n",
              "      <td>Storming msg: Wen u lift d phne, u say \"HELLO\"...</td>\n",
              "      <td>324</td>\n",
              "      <td>[Storming, msg:, Wen, you, lift, d, phne,, you...</td>\n",
              "      <td>Storming msg: Wen you lift d phne, you say \"HE...</td>\n",
              "      <td>[Storming, msg, :, Wen, you, lift, d, phne, ,,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5348</th>\n",
              "      <td>ham</td>\n",
              "      <td>Do I? I thought I put it back in the box</td>\n",
              "      <td>40</td>\n",
              "      <td>[Do, I?, I, thought, I, put, it, back, in, the...</td>\n",
              "      <td>Do I? I thought I put it back in the box</td>\n",
              "      <td>[Do, I, ?, I, thought, I, put, it, back, in, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>749</th>\n",
              "      <td>ham</td>\n",
              "      <td>Is there a reason we've not spoken this year? ...</td>\n",
              "      <td>101</td>\n",
              "      <td>[Is, there, a, reason, we have, not, spoken, t...</td>\n",
              "      <td>Is there a reason we have not spoken this year...</td>\n",
              "      <td>[Is, there, a, reason, we, have, not, spoken, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>279 rows Ã 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     label  ...                                          tokenized\n",
              "1152   ham  ...                   [Sorry, ,, I, will, call, later]\n",
              "1780  spam  ...  [Loan, for, any, purpose, Â£500, -, Â£75,000, .,...\n",
              "901    ham  ...  [Probably, money, worries, ., Things, are, com...\n",
              "3786   ham  ...  [Let, me, know, if, you, need, anything, else,...\n",
              "2396   ham  ...  [Babe, ,, I, am, back, ..., Come, back, to, me...\n",
              "...    ...  ...                                                ...\n",
              "2708  spam  ...  [Great, NEW, Offer, -, DOUBLE, Mins, &, DOUBLE...\n",
              "5026   ham  ...  [I, was, going, to, ask, you, lol, but, i, thi...\n",
              "4456   ham  ...  [Storming, msg, :, Wen, you, lift, d, phne, ,,...\n",
              "5348   ham  ...  [Do, I, ?, I, thought, I, put, it, back, in, t...\n",
              "749    ham  ...  [Is, there, a, reason, we, have, not, spoken, ...\n",
              "\n",
              "[279 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvvhKUHGr_Jt"
      },
      "source": [
        "Clearly there's a lot going on here: digits, gratuitous whitespace, and all varieties of punctuation. Some terms are randomly capitalized, others are in all-caps. Since these terms might show up in any one of the training examples in countless forms, we need a way to ensure each training example is on equal footing via a preprocessing step called normalization. This form of noise cleaning takes care of spacing and any special characters. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-9loSCBsnKL"
      },
      "source": [
        "Transforming all words to lowercase is also a very common pre-processing step. In this case, we will once again append a new column named âlowerâ to the dataframe which will transform all the tokenized words into lowercase. However, because we have to iterate over multiple words we will use a simple for-loop within a lambda function to apply the âlowerâ function to each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGeQt7LCsuQi",
        "outputId": "7bcfb5ce-5d00-4fca-ac89-cd5b3bf6ec69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "sms['lower'] = sms['tokenized'].apply(lambda x: [word.lower() for word in x])\n",
        "sms.head()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "      <th>length</th>\n",
              "      <th>no_contract</th>\n",
              "      <th>msg_str</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lower</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>111</td>\n",
              "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>[Go, until, jurong, point, ,, crazy.., Availab...</td>\n",
              "      <td>[go, until, jurong, point, ,, crazy.., availab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>29</td>\n",
              "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
              "      <td>Ok lar... Joking wif you oni...</td>\n",
              "      <td>[Ok, lar, ..., Joking, wif, you, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>155</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>49</td>\n",
              "      <td>[you, dun, say, so, early, hor..., you, c, alr...</td>\n",
              "      <td>you dun say so early hor... you c already then...</td>\n",
              "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
              "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>61</td>\n",
              "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
              "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
              "      <td>[Nah, I, do, not, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, not, think, he, goes, to, usf, ,,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                              lower\n",
              "0   ham  ...  [go, until, jurong, point, ,, crazy.., availab...\n",
              "1   ham  ...         [ok, lar, ..., joking, wif, you, oni, ...]\n",
              "2  spam  ...  [free, entry, in, 2, a, wkly, comp, to, win, f...\n",
              "3   ham  ...  [you, dun, say, so, early, hor, ..., you, c, a...\n",
              "4   ham  ...  [nah, i, do, not, think, he, goes, to, usf, ,,...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAovmw7AsxCt"
      },
      "source": [
        "Next, we'll remove all punctuation since they serve little value once we begin to analyze our data. Continuing the previous pattern, we will create a new column which has the punctuation removed. We will again utilize a for-loop within a lambda function to iterate over the tokens but this time using an IF condition to only output alpha characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmK_8wVts0ql",
        "outputId": "aaee8c2e-07c3-4753-c91e-c07bab77b8c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "import string\n",
        "punc = string.punctuation\n",
        "sms['no_punc'] = sms['lower'].apply(lambda x: [word for word in x if word not in punc])\n",
        "sms.head()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "      <th>length</th>\n",
              "      <th>no_contract</th>\n",
              "      <th>msg_str</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lower</th>\n",
              "      <th>no_punc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>111</td>\n",
              "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>[Go, until, jurong, point, ,, crazy.., Availab...</td>\n",
              "      <td>[go, until, jurong, point, ,, crazy.., availab...</td>\n",
              "      <td>[go, until, jurong, point, crazy.., available,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>29</td>\n",
              "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
              "      <td>Ok lar... Joking wif you oni...</td>\n",
              "      <td>[Ok, lar, ..., Joking, wif, you, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>155</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>49</td>\n",
              "      <td>[you, dun, say, so, early, hor..., you, c, alr...</td>\n",
              "      <td>you dun say so early hor... you c already then...</td>\n",
              "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
              "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
              "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>61</td>\n",
              "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
              "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
              "      <td>[Nah, I, do, not, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, not, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, not, think, he, goes, to, usf, he...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                            no_punc\n",
              "0   ham  ...  [go, until, jurong, point, crazy.., available,...\n",
              "1   ham  ...         [ok, lar, ..., joking, wif, you, oni, ...]\n",
              "2  spam  ...  [free, entry, in, 2, a, wkly, comp, to, win, f...\n",
              "3   ham  ...  [you, dun, say, so, early, hor, ..., you, c, a...\n",
              "4   ham  ...  [nah, i, do, not, think, he, goes, to, usf, he...\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr24n8hFJm3I"
      },
      "source": [
        " ### Step 4: Spell Checking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsKljmGpv8E6"
      },
      "source": [
        "For spell checking, we will use Microsoft's TextBlob, which is a simple spelling correction mechanism. You can install this using the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJZmd45b0AC0",
        "outputId": "b0ca672a-2e0d-4a25-8e07-da4dd1122429",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "!pip install pyspellchecker"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspellchecker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/96/827c132397d0eb5731c1eda05dbfb019ede064ca8c7d0f329160ce0a4acd/pyspellchecker-0.5.5-py2.py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 1.9MB 2.9MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.5.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfoqLr1EyqQ8"
      },
      "source": [
        "We will first demonstrate a simple example how we implement this spellchecker and how it is able to not only identify misspelled words, but also suggest the most likely corrected spelling along with other likely options."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ_gxY5f0Fwj",
        "outputId": "6237382a-5918-491e-8a2e-ca9bea83b1b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "\n",
        "# find those words that may be misspelled\n",
        "misspelled = spell.unknown(['something', 'is', 'hapenning', 'here'])\n",
        "\n",
        "for word in misspelled:\n",
        "    # Get the one `most likely` answer\n",
        "    print(spell.correction(word))\n",
        "\n",
        "    # Get a list of `likely` options\n",
        "    print(spell.candidates(word))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "happening\n",
            "{'happening', 'henning', 'penning'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9AlDtsX2058"
      },
      "source": [
        "Reference: https://pypi.org/project/pyspellchecker/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKiy3apFJqeb"
      },
      "source": [
        "### Step 6: âStop Wordsâ Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-rMODaGtBlV"
      },
      "source": [
        "Some words in the English language, while necessary, don't contribute much to the meaning of a phrase. These words, such as \"when\", \"had\", \"those\" or \"before\", are called stop words and should be filtered out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGzWZxk-8Pil"
      },
      "source": [
        "First, we need to import the NLTK stopwords library and set our stopwords to âenglishâ. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IlsjVG-LLWX",
        "outputId": "44521aa4-3db3-48aa-d99c-ba537b1ab5fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n73ZUUKrtOcP"
      },
      "source": [
        "We are going to add a new column âno_stopwordsâ which will remove the stopwords from the âno_puncâ column since it has been tokenized, had been converted to lowercase and punctuation was removed. Once again a for-loop within a lambda function will iterate over the tokens in âno_puncâ and only return the tokens which do not exist in our âstop_wordsâ variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMwO1hBltQXZ",
        "outputId": "b59c6a89-08eb-4d0b-c073-bd3c81d45b6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "sms['stopwords_removed'] = sms['no_punc'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "sms.head()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "      <th>length</th>\n",
              "      <th>no_contract</th>\n",
              "      <th>msg_str</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lower</th>\n",
              "      <th>no_punc</th>\n",
              "      <th>stopwords_removed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>111</td>\n",
              "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>[Go, until, jurong, point, ,, crazy.., Availab...</td>\n",
              "      <td>[go, until, jurong, point, ,, crazy.., availab...</td>\n",
              "      <td>[go, until, jurong, point, crazy.., available,...</td>\n",
              "      <td>[go, jurong, point, crazy.., available, bugis,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>29</td>\n",
              "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
              "      <td>Ok lar... Joking wif you oni...</td>\n",
              "      <td>[Ok, lar, ..., Joking, wif, you, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, oni, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>155</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>49</td>\n",
              "      <td>[you, dun, say, so, early, hor..., you, c, alr...</td>\n",
              "      <td>you dun say so early hor... you c already then...</td>\n",
              "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
              "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
              "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
              "      <td>[dun, say, early, hor, ..., c, already, say, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>61</td>\n",
              "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
              "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
              "      <td>[Nah, I, do, not, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, not, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, not, think, he, goes, to, usf, he...</td>\n",
              "      <td>[nah, think, goes, usf, lives, around, though]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                  stopwords_removed\n",
              "0   ham  ...  [go, jurong, point, crazy.., available, bugis,...\n",
              "1   ham  ...              [ok, lar, ..., joking, wif, oni, ...]\n",
              "2  spam  ...  [free, entry, 2, wkly, comp, win, fa, cup, fin...\n",
              "3   ham  ...  [dun, say, early, hor, ..., c, already, say, ...]\n",
              "4   ham  ...     [nah, think, goes, usf, lives, around, though]\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7ZrTjxJJqqn"
      },
      "source": [
        "### Step 6: Stemming/Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPMcw9E7tpqe"
      },
      "source": [
        "The idea of stemming is to reduce different forms of word usage into its root word. For example, âdriveâ, âdroveâ, âdrivingâ, âdrivenâ, âdriverâ are derivatives of the word âdriveâ and very often researchers want to remove this variability from their corpus. Compared to lemmatization, stemming is certainly the less complicated method but it often does not produce a dictionary-specific morphological root of the word. In other words, stemming the word âpiesâ will often produce a root of âpiâ whereas lemmatization will find the morphological root of âpieâ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0jD4XRn_AYM"
      },
      "source": [
        "Instead of taking the easy way out with stemming, letâs apply lemmatization to our data but it requires some additional steps compared to stemming. First, we have to apply parts of speech tags, in other words, determine the part of speech (ie. noun, verb, adverb, etc.) for each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gOKB7uD_Zz8",
        "outputId": "59f84f7d-6ef8-4e45-f2d6-3f1923423b59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX-fBFBdtqCI",
        "outputId": "0b09c85c-c94b-4b58-e7dd-73bf255c5ee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        }
      },
      "source": [
        "sms['pos_tags'] = sms['no_contract'].apply(nltk.tag.pos_tag)\n",
        "sms.head()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "      <th>length</th>\n",
              "      <th>no_contract</th>\n",
              "      <th>msg_str</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lower</th>\n",
              "      <th>no_punc</th>\n",
              "      <th>stopwords_removed</th>\n",
              "      <th>pos_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>111</td>\n",
              "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>[Go, until, jurong, point, ,, crazy.., Availab...</td>\n",
              "      <td>[go, until, jurong, point, ,, crazy.., availab...</td>\n",
              "      <td>[go, until, jurong, point, crazy.., available,...</td>\n",
              "      <td>[go, jurong, point, crazy.., available, bugis,...</td>\n",
              "      <td>[(Go, NNP), (until, IN), (jurong, JJ), (point,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>29</td>\n",
              "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
              "      <td>Ok lar... Joking wif you oni...</td>\n",
              "      <td>[Ok, lar, ..., Joking, wif, you, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, oni, ...]</td>\n",
              "      <td>[(Ok, NNP), (lar..., VBZ), (Joking, NNP), (wif...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>155</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
              "      <td>[(Free, JJ), (entry, NN), (in, IN), (2, CD), (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>49</td>\n",
              "      <td>[you, dun, say, so, early, hor..., you, c, alr...</td>\n",
              "      <td>you dun say so early hor... you c already then...</td>\n",
              "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
              "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
              "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
              "      <td>[dun, say, early, hor, ..., c, already, say, ...]</td>\n",
              "      <td>[(you, PRP), (dun, VBP), (say, VB), (so, RB), ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>61</td>\n",
              "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
              "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
              "      <td>[Nah, I, do, not, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, not, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, not, think, he, goes, to, usf, he...</td>\n",
              "      <td>[nah, think, goes, usf, lives, around, though]</td>\n",
              "      <td>[(Nah, NNP), (I, PRP), (do not, VBP), (think, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                           pos_tags\n",
              "0   ham  ...  [(Go, NNP), (until, IN), (jurong, JJ), (point,...\n",
              "1   ham  ...  [(Ok, NNP), (lar..., VBZ), (Joking, NNP), (wif...\n",
              "2  spam  ...  [(Free, JJ), (entry, NN), (in, IN), (2, CD), (...\n",
              "3   ham  ...  [(you, PRP), (dun, VBP), (say, VB), (so, RB), ...\n",
              "4   ham  ...  [(Nah, NNP), (I, PRP), (do not, VBP), (think, ...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aP4xl6c_eB8"
      },
      "source": [
        "We are going to be using NLTKâs word lemmatizer which needs the parts of speech tags to be converted to wordnetâs format. Weâll write a function which make the proper conversion and then use the function within a list comprehension to apply the conversion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE1a4riM_u2X",
        "outputId": "2b2b80f9-062f-4f32-8cec-cb67db20dea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQcreCGV_hxP"
      },
      "source": [
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17-r9hWP_o0a",
        "outputId": "5225631d-e3df-4a71-ce86-505c2c9a47a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        }
      },
      "source": [
        "sms['wordnet_pos'] = sms['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n",
        "sms.head()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "      <th>length</th>\n",
              "      <th>no_contract</th>\n",
              "      <th>msg_str</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lower</th>\n",
              "      <th>no_punc</th>\n",
              "      <th>stopwords_removed</th>\n",
              "      <th>pos_tags</th>\n",
              "      <th>wordnet_pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>111</td>\n",
              "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>[Go, until, jurong, point, ,, crazy.., Availab...</td>\n",
              "      <td>[go, until, jurong, point, ,, crazy.., availab...</td>\n",
              "      <td>[go, until, jurong, point, crazy.., available,...</td>\n",
              "      <td>[go, jurong, point, crazy.., available, bugis,...</td>\n",
              "      <td>[(Go, NNP), (until, IN), (jurong, JJ), (point,...</td>\n",
              "      <td>[(Go, n), (until, n), (jurong, a), (point,, n)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>29</td>\n",
              "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
              "      <td>Ok lar... Joking wif you oni...</td>\n",
              "      <td>[Ok, lar, ..., Joking, wif, you, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, oni, ...]</td>\n",
              "      <td>[(Ok, NNP), (lar..., VBZ), (Joking, NNP), (wif...</td>\n",
              "      <td>[(Ok, n), (lar..., v), (Joking, n), (wif, n), ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>155</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
              "      <td>[(Free, JJ), (entry, NN), (in, IN), (2, CD), (...</td>\n",
              "      <td>[(Free, a), (entry, n), (in, n), (2, n), (a, n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>49</td>\n",
              "      <td>[you, dun, say, so, early, hor..., you, c, alr...</td>\n",
              "      <td>you dun say so early hor... you c already then...</td>\n",
              "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
              "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
              "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
              "      <td>[dun, say, early, hor, ..., c, already, say, ...]</td>\n",
              "      <td>[(you, PRP), (dun, VBP), (say, VB), (so, RB), ...</td>\n",
              "      <td>[(you, n), (dun, v), (say, v), (so, r), (early...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>61</td>\n",
              "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
              "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
              "      <td>[Nah, I, do, not, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, not, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, not, think, he, goes, to, usf, he...</td>\n",
              "      <td>[nah, think, goes, usf, lives, around, though]</td>\n",
              "      <td>[(Nah, NNP), (I, PRP), (do not, VBP), (think, ...</td>\n",
              "      <td>[(Nah, n), (I, n), (do not, v), (think, v), (h...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                        wordnet_pos\n",
              "0   ham  ...  [(Go, n), (until, n), (jurong, a), (point,, n)...\n",
              "1   ham  ...  [(Ok, n), (lar..., v), (Joking, n), (wif, n), ...\n",
              "2  spam  ...  [(Free, a), (entry, n), (in, n), (2, n), (a, n...\n",
              "3   ham  ...  [(you, n), (dun, v), (say, v), (so, r), (early...\n",
              "4   ham  ...  [(Nah, n), (I, n), (do not, v), (think, v), (h...\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCjEH6jG_iCE"
      },
      "source": [
        "Now we can apply NLTKâs word lemmatizer within our trusty list comprehension. Notice, the lemmatizer function requires two parameters the word and its tag (in wordnet form)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96mw3ww8AGNx"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJtg03P3_iXA",
        "outputId": "7099118e-c8e4-428d-ace3-f413edac884f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        }
      },
      "source": [
        "wnl = WordNetLemmatizer()\n",
        "sms['lemmatized'] = sms['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n",
        "sms.head()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "      <th>length</th>\n",
              "      <th>no_contract</th>\n",
              "      <th>msg_str</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lower</th>\n",
              "      <th>no_punc</th>\n",
              "      <th>stopwords_removed</th>\n",
              "      <th>pos_tags</th>\n",
              "      <th>wordnet_pos</th>\n",
              "      <th>lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>111</td>\n",
              "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>[Go, until, jurong, point, ,, crazy.., Availab...</td>\n",
              "      <td>[go, until, jurong, point, ,, crazy.., availab...</td>\n",
              "      <td>[go, until, jurong, point, crazy.., available,...</td>\n",
              "      <td>[go, jurong, point, crazy.., available, bugis,...</td>\n",
              "      <td>[(Go, NNP), (until, IN), (jurong, JJ), (point,...</td>\n",
              "      <td>[(Go, n), (until, n), (jurong, a), (point,, n)...</td>\n",
              "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>29</td>\n",
              "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
              "      <td>Ok lar... Joking wif you oni...</td>\n",
              "      <td>[Ok, lar, ..., Joking, wif, you, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, oni, ...]</td>\n",
              "      <td>[(Ok, NNP), (lar..., VBZ), (Joking, NNP), (wif...</td>\n",
              "      <td>[(Ok, n), (lar..., v), (Joking, n), (wif, n), ...</td>\n",
              "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>155</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
              "      <td>[(Free, JJ), (entry, NN), (in, IN), (2, CD), (...</td>\n",
              "      <td>[(Free, a), (entry, n), (in, n), (2, n), (a, n...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>49</td>\n",
              "      <td>[you, dun, say, so, early, hor..., you, c, alr...</td>\n",
              "      <td>you dun say so early hor... you c already then...</td>\n",
              "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
              "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
              "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
              "      <td>[dun, say, early, hor, ..., c, already, say, ...]</td>\n",
              "      <td>[(you, PRP), (dun, VBP), (say, VB), (so, RB), ...</td>\n",
              "      <td>[(you, n), (dun, v), (say, v), (so, r), (early...</td>\n",
              "      <td>[you, dun, say, so, early, hor..., you, c, alr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>61</td>\n",
              "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
              "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
              "      <td>[Nah, I, do, not, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, not, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, not, think, he, goes, to, usf, he...</td>\n",
              "      <td>[nah, think, goes, usf, lives, around, though]</td>\n",
              "      <td>[(Nah, NNP), (I, PRP), (do not, VBP), (think, ...</td>\n",
              "      <td>[(Nah, n), (I, n), (do not, v), (think, v), (h...</td>\n",
              "      <td>[Nah, I, do not, think, he, go, to, usf,, he, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                         lemmatized\n",
              "0   ham  ...  [Go, until, jurong, point,, crazy.., Available...\n",
              "1   ham  ...             [Ok, lar..., Joking, wif, you, oni...]\n",
              "2  spam  ...  [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
              "3   ham  ...  [you, dun, say, so, early, hor..., you, c, alr...\n",
              "4   ham  ...  [Nah, I, do not, think, he, go, to, usf,, he, ...\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpqlSX_JASXz"
      },
      "source": [
        "Lastly, we should save all of our pre-processing work for the next steps in the workflow. We can simnple save it as a csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRKemvFgAc4S"
      },
      "source": [
        "sms.to_csv('sms_spam_collection.csv')"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQweWcQorWs6"
      },
      "source": [
        "References:\n",
        "- https://github.com/ujjwalgupta07/spam_ham_detection/blob/master/spam_ham_classification.ipynb\n",
        "- https://archive.ics.uci.edu/ml/datasets/sms+spam+collection\n",
        "- https://www.datacamp.com/community/tutorials/exploratory-data-analysis-python\n",
        "- https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/#d-removing-contractions\n",
        "\n",
        "***\n",
        "- https://stackabuse.com/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library/\n",
        "- https://inmachineswetrust.com/posts/sms-spam-filter/\n",
        "- https://realpython.com/natural-language-processing-spacy-python/\n",
        "- https://github.com/WomenWhoCode/WWCodeDataScience/blob/master/Intro_to_MachineLearning/1_Introduction.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3_uGCKBXV6z"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}