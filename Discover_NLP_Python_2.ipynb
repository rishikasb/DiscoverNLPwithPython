{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Discover_NLP_Python#2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zJB0jAqL9xOl",
        "o62inOGq95XL",
        "MvvcehGy-AlT"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n139TJKb1GvD"
      },
      "source": [
        "# Discover NLP with Python Part II\n",
        "\n",
        "\n",
        "Last time we performed Exploratory Data Analysis(EDA) to understand the specifics of our dataset. We plotted information to see underlying information.\n",
        "\n",
        "We also learned basic text processing techniques including tokenization, lemmatization, and stemming to prepare text before inputting that to our model. \n",
        "\n",
        "\n",
        "This session we would take forward what we learned in last session to prepare a basic sentiment analysis model. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVeUlPht4EiF"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "We are picking the US Airlines sentiment dataset from Kaggle. The dataset contains customer reviews on Twitter regarding 6 US Airlines.\n",
        "\n",
        "There are three sentiments: Positive, Negative and Neutral.\n",
        "\n",
        "Our task is to analyze the reviews, find the reasons behind negative reviews and classify unseen reviews in the correct catgory. \n",
        "\n",
        "\n",
        "Find the dataset here: https://www.kaggle.com/crowdflower/twitter-airline-sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjsVKbkS6W2C"
      },
      "source": [
        "Let's get started with importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wLbH3UV9KIK",
        "outputId": "6c563970-2a2d-446a-93b6-a6083667564e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "!pip install kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (0.0.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.6.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhrjFSHa5xeB"
      },
      "source": [
        "### Downloading the dataset from  Kaggle\n",
        "\n",
        "We can directly download dataset from Kaggle in our Colab notebook.\n",
        "\n",
        "The steps are:\n",
        "\n",
        "- Create an account on Kaggle\n",
        "- Go to your profile, generate new API token\n",
        "- Set permissions and download the dataset using API. By now you'd see the zip file\n",
        "- To access the csv file, unzip the dataset using `!unzip` command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO43tMjd6sDF"
      },
      "source": [
        "# Upload your Kaggle API in order to download the required dataset\n",
        "from google.colab import files\n",
        "files.upload()  #this will prompt you to upload the kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wofxyfx_6twi"
      },
      "source": [
        "# Preparing the API for downloading\n",
        "!mkdir -p ~/.kaggle \n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yov1GD9g61XL",
        "outputId": "61128434-b3b1-4622-a5e6-13e204b20686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "!pip show kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: kaggle\n",
            "Version: 1.5.8\n",
            "Summary: Kaggle API\n",
            "Home-page: https://github.com/Kaggle/kaggle-api\n",
            "Author: Kaggle\n",
            "Author-email: support@kaggle.com\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: requests, tqdm, urllib3, certifi, python-slugify, six, slugify, python-dateutil\n",
            "Required-by: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZbquT5i63iy",
        "outputId": "998270e5-d783-4822-b3d0-12921eda0041",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!kaggle datasets download -d crowdflower/twitter-airline-sentiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading twitter-airline-sentiment.zip to /content\n",
            "\r  0% 0.00/2.55M [00:00<?, ?B/s]\n",
            "\r100% 2.55M/2.55M [00:00<00:00, 85.1MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vvOIy8X65ML",
        "outputId": "96988b70-191b-4837-b93d-3858455b69c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!unzip /content/twitter-airline-sentiment.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/twitter-airline-sentiment.zip\n",
            "  inflating: Tweets.csv              \n",
            "  inflating: database.sqlite         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k5sZ-Tj8k2K"
      },
      "source": [
        "## Analyzing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLnz4r4z8G30"
      },
      "source": [
        "#reading data\n",
        "data = pd.read_csv(\"Tweets.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3RNYUWA8myG",
        "outputId": "c9f31faa-946f-4ed4-fa35-9f841ca0c7ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#checking shape of the dataset\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14640, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY3CTtUr9OMZ",
        "outputId": "b99d5189-b18d-4a73-ee84-a2a431c31fde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:35:52 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yvonnalynn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:48 -0800</td>\n",
              "      <td>Lets Play</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>0.7033</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:36 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:14:45 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id  ...               user_timezone\n",
              "0  570306133677760513  ...  Eastern Time (US & Canada)\n",
              "1  570301130888122368  ...  Pacific Time (US & Canada)\n",
              "2  570301083672813571  ...  Central Time (US & Canada)\n",
              "3  570301031407624196  ...  Pacific Time (US & Canada)\n",
              "4  570300817074462722  ...  Pacific Time (US & Canada)\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRtr7Su39Yn2"
      },
      "source": [
        "#Let's know more about the data using data.describe.\n",
        "\n",
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVvr5S7e9gdY"
      },
      "source": [
        "\n",
        "Checking for the null values. Some rows like negativereason_confidence mostly consists of empty rows, imputing them will not help. So we will drop these columns later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwJhVw7w9c4e"
      },
      "source": [
        "data.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY-d_3ZS9kbi"
      },
      "source": [
        "Let's see how sentiments are distributed that is number of samples per sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMc1X-oU9iSz"
      },
      "source": [
        "data['airline_sentiment'].value_counts().plot(kind='bar', color=['red','yellow','green'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2zwSLX19o2f"
      },
      "source": [
        "\n",
        "Negative sentiments samples are much more than the negative and positive sentiments. This indicates at the imablanced data.\n",
        "\n",
        "Let's see the number of samples for all the airlines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk_X00Xx9l7u"
      },
      "source": [
        "data['airline'].value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WND_vXv9uZZ"
      },
      "source": [
        "\n",
        "People have tweeted the most for United followed by US Airways. We'll see if these tweets were postive, negative or neutral."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJB0jAqL9xOl"
      },
      "source": [
        "## Sentiments per Airline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYhU7qnu9qm2"
      },
      "source": [
        "pd.crosstab(data['airline'],data['airline_sentiment']).plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM8e_wR29y_T"
      },
      "source": [
        "data['airline'].groupby(data['airline_sentiment']).value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKV635sE94kG"
      },
      "source": [
        "\n",
        "Now we will see how individual sentiments are distrubuted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o62inOGq95XL"
      },
      "source": [
        "## Positive Sentiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAgF7q0391q-"
      },
      "source": [
        "data[data['airline_sentiment']== 'positive'].airline.value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvvcehGy-AlT"
      },
      "source": [
        "## Negative Sentiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpv6Q_-_9_5p"
      },
      "source": [
        "data[data['airline_sentiment']== 'negative'].airline.value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZh1gWLM-GAp"
      },
      "source": [
        "## Neutral Sentiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhKDbpa399Dj"
      },
      "source": [
        "data[data['airline_sentiment']== 'neutral'].airline.value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96najjQj-J8G"
      },
      "source": [
        "\n",
        "So clearly people are unahppy with United. Most negative tweets are for united.\n",
        "\n",
        "Southwest has done a good job in serving people and hence it got most positive tweets.\n",
        "\n",
        "Whereas Virgin America is stable or (least popular?) with balanced negative, positive and neutral tweets.\n",
        "\n",
        "We will now visualize the negative reasons for negative sentiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-UQmF04-Ei1"
      },
      "source": [
        "data['negativereason'].value_counts().plot(kind='barh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_Clpp3Z-O2n"
      },
      "source": [
        "So mostly, Customer Service Issue made people unhappy. If airlines want to do better they should focus on Customer Service.\n",
        "\n",
        "Negative reason per airline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPdimHNx-LwK"
      },
      "source": [
        "pd.crosstab(data['airline'], data['negativereason'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRLBfrgg-REk"
      },
      "source": [
        "pd.crosstab(data['airline'], data['negativereason']).plot(kind='bar')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHHOt5ET-U1F"
      },
      "source": [
        "\n",
        "American is guilty of the worst Customer Service Issue. Remebering United got the most negative sentiments, the reason for that seems to be the same i.e. Customer Service Issue followed by Late Flight.\n",
        "\n",
        "Visualizing the correlation in the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnyMA-hR-S2V"
      },
      "source": [
        "sns.heatmap(data.corr())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS2YrtSL-Yr9"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "We are going to pick 'airline_sentiment','text' rows for our task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNaaPn0V-WsR"
      },
      "source": [
        "df=data[['airline_sentiment','text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gie8KuU--cnw"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_YRDwT4-e_j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRwNzghz-qKM"
      },
      "source": [
        "\n",
        "Since the data is collected from twitter it is obvious to find links and mentions in the data which are not helpful in the analysis so we will remove them.\n",
        "\n",
        "For preprocessing the text we will remove all of the following:\n",
        "\n",
        "*  stopwords\n",
        "*  punctuations\n",
        "*  links\n",
        "*  mentions(@)\n",
        "\n",
        "We will also perform stemming with the help of nltk library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGSpt9BL-iHu",
        "outputId": "fad0bb33-051e-428b-9130-3a95a7bfb9aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# Preprocessing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop=stopwords.words('english')\n",
        "from nltk import word_tokenize\n",
        "import regex as re\n",
        "snow=nltk.stem.SnowballStemmer('english')\n",
        "\n",
        "def preprocess(doc):\n",
        "  doc=re.sub('@\\w+',\" \",str(doc))\n",
        "  doc=re.sub('#\\w+',\" \",str(doc))\n",
        "  doc=re.sub('http\\S+',\" \",str(doc))\n",
        "  doc=re.sub('[^\\w\\s]',\" \",str(doc)) \n",
        "  doc=re.sub('[^a-zA-Z]',\" \",str(doc))\n",
        "  tokens=word_tokenize(doc)\n",
        "  word=[snow.stem(word) for word in tokens]\n",
        "  word=[word for word in tokens if word not in stop]\n",
        "  word = [w.lower() for w in word]\n",
        "  words='  '.join(word)\n",
        "  return words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIM5xHjW-iuj",
        "outputId": "0406bc9d-212e-447e-c7c2-73ff37ed0e48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "df.text.apply(preprocess)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                               what  said\n",
              "1              plus  added  commercials  experience  tacky\n",
              "2        i  today  must  mean  i  need  take  another  ...\n",
              "3        really  aggressive  blast  obnoxious  entertai...\n",
              "4                                  really  big  bad  thing\n",
              "                               ...                        \n",
              "14635               thank  got  different  flight  chicago\n",
              "14636    leaving  minutes  late  flight  no  warnings  ...\n",
              "14637                    please  bring  american  airlines\n",
              "14638    money  change  flight  answer  phones  any  su...\n",
              "14639    ppl  need  know  many  seats  next  flight  pl...\n",
              "Name: text, Length: 14640, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5j8QRyK-tAC"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdoCFUAY-yso"
      },
      "source": [
        "Let's plot the wordcloud and see how the words are distributed and how the overall data looks like.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uapXUvBC-vcM"
      },
      "source": [
        "from wordcloud import WordCloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I99lyLZ-xKM"
      },
      "source": [
        "  df.text = df.text.astype(str)\n",
        "  all_words = ' '.join(text for text in df.text)\n",
        "\n",
        "  wordcloud_obj = WordCloud(width= 800,\n",
        "                            height= 500, \n",
        "                            max_font_size= 110, \n",
        "                            collocations= False).generate(all_words)\n",
        "\n",
        "  plt.figure(figsize=(15,10))\n",
        "  plt.imshow(wordcloud_obj, interpolation= \"bilinear\")\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmBehJ8d-401"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odVlT_vv_OUF"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10nJFg1f_Q5h"
      },
      "source": [
        "Before sending the data to our model we have to convert text into numerical form. \n",
        "\n",
        "In the presentation we saw how we can use Bag of Words for this purpose but we are going to Tf-Idf instead. \n",
        "\n",
        "The reason behind that is, Bag of Words regards each word as unique when it calculates the frequency, hence a large sparse array is created. This is not possible to use practically.\n",
        "\n",
        "Tf-Idf calculates the importance of each word based on its occurrence in the document and is faster than Bag of Words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl9tqW85_Nm0"
      },
      "source": [
        "#importing TfidfVectorizer from scikit-lear\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF5rWwcz_p_r"
      },
      "source": [
        "tfidf=TfidfVectorizer()\n",
        "reviews = tfidf.fit_transform(df['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umaZLqqXnXGS"
      },
      "source": [
        "Now let's convert our labels 'Positive', 'Negative' and 'Neutral' in numerical form. \n",
        "\n",
        "For this we are going to use scikit-learn's Label Encoder().\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn3-2mHEnng0"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le=LabelEncoder()\n",
        "labels=le.fit_transform(df['airline_sentiment'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUtkZ9vuBGNb"
      },
      "source": [
        "## Splitiing the dataset in training and testing set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyskiZStAksH"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.25, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5Wbf4A4Bu0I"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "We can use any machine learning algorithm for this task, and we have chosen the Support Vector Classifier for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcnmwOyiAmEI"
      },
      "source": [
        "#importing SVC from scikit-learn\n",
        "from sklearn.svm import SVC\n",
        "text_classifier = SVC(random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ0AXmixC8uL",
        "outputId": "43152254-cdb6-48e8-9c99-c392ff0927c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "text_classifier.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=0, shrinking=True, tol=0.001,\n",
              "    verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAgDjpuMB06m"
      },
      "source": [
        "## Predictions\n",
        "\n",
        "Let's do some predictions on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvLg5udHBwP-"
      },
      "source": [
        "predictions = text_classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLXzQoCnyhYW",
        "outputId": "3ac45ea8-1654-444f-a0f6-1f8595066681",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "#comparing test_labels and predicted label\n",
        "dataframe=pd.DataFrame()\n",
        "dataframe['y_test']=le.inverse_transform(y_test)\n",
        "dataframe['Predicted']=le.inverse_transform(predictions)\n",
        "dataframe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y_test</th>\n",
              "      <th>Predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3655</th>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3656</th>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3657</th>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3658</th>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3659</th>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3660 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        y_test Predicted\n",
              "0     negative  negative\n",
              "1     negative  negative\n",
              "2     negative  negative\n",
              "3     negative  negative\n",
              "4     negative  positive\n",
              "...        ...       ...\n",
              "3655  negative  negative\n",
              "3656  negative  negative\n",
              "3657  positive  negative\n",
              "3658  positive  positive\n",
              "3659  negative  negative\n",
              "\n",
              "[3660 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZESKlqIB2mC"
      },
      "source": [
        "## Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IznlRmheBybD",
        "outputId": "a1b23471-d456-42be-b728-1289128732cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "print(confusion_matrix(y_test,predictions))\n",
        "print(classification_report(y_test,predictions))\n",
        "print(accuracy_score(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2227   80   20]\n",
            " [ 363  378   31]\n",
            " [ 180   71  310]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.96      0.87      2327\n",
            "           1       0.71      0.49      0.58       772\n",
            "           2       0.86      0.55      0.67       561\n",
            "\n",
            "    accuracy                           0.80      3660\n",
            "   macro avg       0.79      0.67      0.71      3660\n",
            "weighted avg       0.79      0.80      0.78      3660\n",
            "\n",
            "0.796448087431694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp3HtIpvrpQH"
      },
      "source": [
        "We got a reasonable accuracy of 79%, this could be further improved by using deep learning techniques and language models. \n",
        "\n",
        "We are going to cover that in the future sessions, the purpose of this session is to show you the complete pipeline of a NLP project and get you comfortable with text manipulation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sznvddaLzP4L"
      },
      "source": [
        "References:\n",
        "\n",
        "1. [Kaggle dataset in Colab](https://www.geeksforgeeks.org/importing-kaggle-dataset-into-google-colaboratory/)\n",
        "\n",
        "2. [TfIdfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alihjq1AsWEQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}